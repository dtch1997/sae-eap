{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rough notebook to try doing EAP with existing graph stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List, Optional\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from transformer_lens import HookedTransformer\n",
    "from tqdm import tqdm\n",
    "from einops import einsum\n",
    "\n",
    "from sae_eap.graph import Graph, InputNode, LogitNode, AttentionNode\n",
    "\n",
    "\n",
    "def get_npos_input_lengths(model, inputs):\n",
    "    tokenized = model.tokenizer(\n",
    "        inputs, padding=\"longest\", return_tensors=\"pt\", add_special_tokens=True\n",
    "    )\n",
    "    n_pos = 1 + tokenized.attention_mask.size(1)\n",
    "    input_lengths = 1 + tokenized.attention_mask.sum(1)\n",
    "    return n_pos, input_lengths\n",
    "\n",
    "\n",
    "def make_hooks_and_matrices(\n",
    "    model: HookedTransformer, graph: Graph, batch_size: int, n_pos: int\n",
    "):\n",
    "    \"\"\"Returns hooks for forward and backward passes, and matrices for storing activations and gradients\"\"\"\n",
    "\n",
    "    # Initialize tensors\n",
    "    activation_difference = torch.zeros(\n",
    "        (batch_size, n_pos, graph.n_forward_nodes, model.cfg.d_model),\n",
    "        device=\"cuda\",\n",
    "        dtype=model.cfg.dtype,\n",
    "    )\n",
    "    gradients = torch.zeros(\n",
    "        (batch_size, n_pos, graph.n_backward_nodes, model.cfg.d_model),\n",
    "        device=\"cuda\",\n",
    "        dtype=model.cfg.dtype,\n",
    "    )\n",
    "\n",
    "    # Initialize hooks\n",
    "    fwd_hooks_clean = []\n",
    "    fwd_hooks_corrupted = []\n",
    "    bwd_hooks = []\n",
    "\n",
    "    # Define a hook function to store a tensor\n",
    "    # When doing clean forward pass, we'll want to add activations to the tensor\n",
    "    # When doing corrupted forward pass, we'll want to subtract activations from the tensor\n",
    "    def activation_hook(t: torch.Tensor, index, add=True):\n",
    "        def hook_fn(activations, hook):\n",
    "            acts = activations.detach()\n",
    "            try:\n",
    "                if add:\n",
    "                    t[index] += acts\n",
    "                else:\n",
    "                    t[index] -= acts\n",
    "            except RuntimeError as e:\n",
    "                print(hook.name, t.size(), acts.size())\n",
    "                raise e\n",
    "\n",
    "        return hook_fn\n",
    "\n",
    "    processed_attn_layers = set()\n",
    "    for node in graph.nodes:\n",
    "        # NOTE: We batch-process all attention nodes in a layer at once\n",
    "        # So we need to keep track of which layers we've already processed\n",
    "        # and skip all subsequent attention nodes in that layer\n",
    "        # TODO: Extract as function\n",
    "        # def if_processed(node):\n",
    "        if isinstance(node, AttentionNode):\n",
    "            if node.layer in processed_attn_layers:\n",
    "                continue\n",
    "            else:\n",
    "                processed_attn_layers.add(node.layer)\n",
    "\n",
    "        # Define the forward hooks\n",
    "        # - For each forward node, we need to add its activation to each child\n",
    "        # - This means that each node will contain the sum of gradients from all parents\n",
    "        # TODO: Extract as function\n",
    "        # def maybe_add_forward_hook(node):\n",
    "        if not isinstance(node, LogitNode):\n",
    "            fwd_index = (slice(None), slice(None), graph.forward_index(node))\n",
    "            fwd_hooks_corrupted.append(\n",
    "                (node.out_hook, activation_hook(activation_difference, fwd_index))\n",
    "            )\n",
    "            fwd_hooks_clean.append(\n",
    "                (\n",
    "                    node.out_hook,\n",
    "                    activation_hook(activation_difference, fwd_index, add=False),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Define the backward hooks.\n",
    "        # - For every backward node, we need to add its gradient to each parent\n",
    "        # - This means that each node will contain the sum of gradients from all children\n",
    "        # TODO: Extract as function\n",
    "        # def maybe_add_backward_hook(node):\n",
    "        if not isinstance(node, InputNode):\n",
    "            if isinstance(node, AttentionNode):\n",
    "                for i, letter in enumerate(\"qkv\"):\n",
    "                    bwd_index = (\n",
    "                        slice(None),\n",
    "                        slice(None),\n",
    "                        graph.backward_index(node, qkv=letter),\n",
    "                    )\n",
    "                    bwd_hooks.append(\n",
    "                        (node.qkv_inputs[i], activation_hook(gradients, bwd_index))\n",
    "                    )\n",
    "            else:\n",
    "                bwd_index = (slice(None), slice(None), graph.backward_index(node))\n",
    "                bwd_hooks.append((node.in_hook, activation_hook(gradients, bwd_index)))\n",
    "\n",
    "    return (fwd_hooks_corrupted, fwd_hooks_clean, bwd_hooks), (\n",
    "        activation_difference,\n",
    "        gradients,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_activations(\n",
    "    model: HookedTransformer,\n",
    "    graph: Graph,\n",
    "    clean_inputs: List[str],\n",
    "    corrupted_inputs: List[str],\n",
    "    metric: Callable[[Tensor], Tensor],\n",
    "    labels,\n",
    "):\n",
    "    batch_size = len(clean_inputs)\n",
    "    n_pos, input_lengths = get_npos_input_lengths(model, clean_inputs)\n",
    "\n",
    "    (\n",
    "        (fwd_hooks_corrupted, fwd_hooks_clean, bwd_hooks),\n",
    "        (activation_difference, gradients),\n",
    "    ) = make_hooks_and_matrices(model, graph, batch_size, n_pos)\n",
    "\n",
    "    with model.hooks(fwd_hooks=fwd_hooks_corrupted):\n",
    "        corrupted_logits = model(corrupted_inputs)\n",
    "\n",
    "    with model.hooks(fwd_hooks=fwd_hooks_clean, bwd_hooks=bwd_hooks):\n",
    "        logits = model(clean_inputs)\n",
    "        metric_value = metric(logits, corrupted_logits, input_lengths, labels)\n",
    "        metric_value.backward()\n",
    "\n",
    "    return activation_difference, gradients\n",
    "\n",
    "\n",
    "def get_activations_ig(\n",
    "    model: HookedTransformer,\n",
    "    graph: Graph,\n",
    "    clean_inputs: List[str],\n",
    "    corrupted_inputs: List[str],\n",
    "    metric: Callable[[Tensor], Tensor],\n",
    "    labels,\n",
    "    steps=30,\n",
    "):\n",
    "    batch_size = len(clean_inputs)\n",
    "    n_pos, input_lengths = get_npos_input_lengths(model, clean_inputs)\n",
    "\n",
    "    (\n",
    "        (fwd_hooks_corrupted, fwd_hooks_clean, bwd_hooks),\n",
    "        (activation_difference, gradients),\n",
    "    ) = make_hooks_and_matrices(model, graph, batch_size, n_pos)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        with model.hooks(fwd_hooks=fwd_hooks_corrupted):\n",
    "            _ = model(corrupted_inputs)\n",
    "\n",
    "        input_activations_corrupted = activation_difference[\n",
    "            :, :, graph.forward_index(graph.nodes[\"input\"])\n",
    "        ].clone()\n",
    "\n",
    "        with model.hooks(fwd_hooks=fwd_hooks_clean):\n",
    "            clean_logits = model(clean_inputs)\n",
    "\n",
    "        input_activations_clean = (\n",
    "            input_activations_corrupted\n",
    "            - activation_difference[:, :, graph.forward_index(graph.nodes[\"input\"])]\n",
    "        )\n",
    "\n",
    "    def input_interpolation_hook(k: int):\n",
    "        def hook_fn(activations, hook):\n",
    "            new_input = input_activations_corrupted + (k / steps) * (\n",
    "                input_activations_clean - input_activations_corrupted\n",
    "            )\n",
    "            new_input.requires_grad = True\n",
    "            return new_input\n",
    "\n",
    "        return hook_fn\n",
    "\n",
    "    total_steps = 0\n",
    "    for step in range(1, steps + 1):\n",
    "        total_steps += 1\n",
    "        with model.hooks(\n",
    "            fwd_hooks=[(graph.nodes[\"input\"].out_hook, input_interpolation_hook(step))],\n",
    "            bwd_hooks=bwd_hooks,\n",
    "        ):\n",
    "            logits = model(clean_inputs)\n",
    "            metric_value = metric(logits, clean_logits, input_lengths, labels)\n",
    "            metric_value.backward()\n",
    "\n",
    "    gradients = gradients / total_steps\n",
    "\n",
    "    return activation_difference, gradients\n",
    "\n",
    "\n",
    "allowed_aggregations = {\"sum\", \"mean\", \"l2\"}\n",
    "\n",
    "\n",
    "def attribute(\n",
    "    model: HookedTransformer,\n",
    "    graph: Graph,\n",
    "    dataloader: DataLoader,\n",
    "    metric: Callable[[Tensor], Tensor],\n",
    "    aggregation=\"sum\",\n",
    "    integrated_gradients: Optional[int] = None,\n",
    "    quiet=False,\n",
    "):\n",
    "    if aggregation not in allowed_aggregations:\n",
    "        raise ValueError(\n",
    "            f\"aggregation must be in {allowed_aggregations}, but got {aggregation}\"\n",
    "        )\n",
    "\n",
    "    all_scores = torch.zeros(\n",
    "        (graph.n_forward, graph.n_backward), device=\"cuda\", dtype=model.cfg.dtype\n",
    "    )\n",
    "\n",
    "    total_items = 0\n",
    "    dataloader = dataloader if quiet else tqdm(dataloader)\n",
    "    for clean, corrupted, label in dataloader:\n",
    "        batch_size = len(clean)\n",
    "        total_items += batch_size\n",
    "\n",
    "        if integrated_gradients is None:\n",
    "            activation_differences, gradients = get_activations(\n",
    "                model, graph, clean, corrupted, metric, label\n",
    "            )\n",
    "        else:\n",
    "            assert (\n",
    "                integrated_gradients > 0\n",
    "            ), f\"integrated_gradients gives positive # steps (m), but got {integrated_gradients}\"\n",
    "            activation_differences, gradients = get_activations_ig(\n",
    "                model,\n",
    "                graph,\n",
    "                clean,\n",
    "                corrupted,\n",
    "                metric,\n",
    "                label,\n",
    "                steps=integrated_gradients,\n",
    "            )\n",
    "\n",
    "        scores = einsum(\n",
    "            activation_differences,\n",
    "            gradients,\n",
    "            \"batch pos n_forward hidden, batch pos n_backward hidden -> n_forward n_backward\",\n",
    "        )\n",
    "\n",
    "        if aggregation == \"mean\":\n",
    "            scores /= model.cfg.d_model\n",
    "        elif aggregation == \"l2\":\n",
    "            scores = torch.linalg.vector_norm(scores, ord=2, dim=-1)\n",
    "\n",
    "        all_scores += scores\n",
    "    all_scores /= total_items\n",
    "    all_scores = all_scores.cpu().numpy()\n",
    "\n",
    "    for edge in tqdm(graph.edges.values(), total=len(graph.edges)):\n",
    "        edge.score = all_scores[\n",
    "            graph.forward_index(edge.parent, attn_slice=False),\n",
    "            graph.backward_index(edge.child, qkv=edge.qkv, attn_slice=False),\n",
    "        ]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
